python train.py \
    --output_dir ./model \
    --cleanup_output_dir_on_start \
    --max_num_samples 100 \
    --half_precision_backend cuda_amp \
    --model_id NousResearch/Llama-2-7b-hf \
    --report_to_mlfoundry true \
    --ml_repo llama-finetune-test \
    --train_data artifact:truefoundry/llama-finetune-test/data:1 \
    --eval_data artifact:truefoundry/llama-finetune-test/data:1 \
    --max_num_samples 1000 \
    --eval_size 0.1 \
    --num_train_epochs 3 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --learning_rate 0.00005 \
    --warmup_ratio 0.3 \
    --gradient_accumulation_steps 1 \
    --logging_steps 0.1 \
    --logging_strategy steps\
    --seed 42 \
    --data_seed 42 \
    --lr_scheduler_type linear \
    --weight_decay 0.01 \
    --max_grad_norm 1.0 \
    --gradient_checkpointing true \
    --save_total_limit 3 \
    --use_qlora true \
    --lora_config '{"r": 8, "lora_alpha": 32, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"], "lora_dropout": 0.05, "bias": "none", "task_type": "CAUSAL_LM"}'





python train.py --model_id NousResearch/Llama-2-7b-hf --output_dir model/ --report_to_mlfoundry true --ml_repo llama-finetune-test --train_data artifact:truefoundry/llama-finetune-test/data:1 --eval_data artifact:truefoundry/llama-finetune-test/data:1 --eval_size 0.1 --max_num_samples 1000 --num_train_epochs 3 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --learning_rate 0.00005 --warmup_ratio 0.3 --gradient_accumulation_steps 1 --logging_steps 0.1 --logging_strategy steps --seed 42 --data_seed 42 --lr_scheduler_type linear --weight_decay 0.01 --max_grad_norm 1.0 --gradient_checkpointing true --save_strategy epoch --save_steps 500 --evaluation_strategy epoch --eval_steps 0.1 --log_checkpoints_to_mlfoundry true --use_qlora true --lora_config '{"r":8,"lora_alpha":32,"target_modules":["q_proj","k_proj","v_proj","o_proj"],"lora_dropout":0.05,"task_type":"CAUSAL_LM"}' --half_precision_backend cuda_amp