{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truefoundry/truefoundry-examples/blob/main/sample-notebooks/iris-tfy.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJyTFPXrH3Uo"
      },
      "source": [
        "# Iris Classification with TrueFoundry\n",
        "\n",
        "More details about the dataset: https://archive.ics.uci.edu/ml/datasets/iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRa4ej5bBXi8"
      },
      "source": [
        "## Install TrueFoundry libraries\n",
        "\n",
        "1. MLFoundry - for tracking ML experiments\n",
        "2. ServiceFoundry - to deploy applications from trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn6C36497ifC",
        "outputId": "2fd7f587-aa9d-49af-ed1d-283d1fda839e"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"mlfoundry>=0.3.33,<0.4.0\"\n",
        "!pip install -U servicefoundry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC0SzznK757W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d8R9C8aCBr6"
      },
      "source": [
        "## Create an MLFoundry client and create a run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xLVD_zOMYDb",
        "outputId": "0553a2da-ead9-4309-86b9-780a38d65755"
      },
      "outputs": [],
      "source": [
        "import mlfoundry as mlf\n",
        "mlf.login()\n",
        "client = mlf.get_client()\n",
        "\n",
        "ML_REPO = 'iris-classification-project-1900'\n",
        "run = client.create_run(ml_repo=ML_REPO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TteRmtkJCGV5"
      },
      "source": [
        "## Split datasets into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjVvUu0y7_QV"
      },
      "outputs": [],
      "source": [
        "data = datasets.load_iris()\n",
        "\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A-OrDuCUgxR",
        "outputId": "637856b7-110b-45b9-fcc1-c7960849427e"
      },
      "outputs": [],
      "source": [
        "print(X.head())\n",
        "print(data.target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAxCjkZyCM4k"
      },
      "source": [
        "## Initialise the model and log params to MLFoundry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZP3KZhO8GpV",
        "outputId": "bfe60802-cf70-425b-d4c5-4d9d371295a9"
      },
      "outputs": [],
      "source": [
        "clf = SVC(gamma='scale', kernel='rbf', probability=True, C=1.2)\n",
        "run.set_tags({'framework': 'sklearn', 'task': 'classification'})\n",
        "run.log_params(clf.get_params())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeiYN3CjCSc8"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06JbRQpLNQol",
        "outputId": "ee7e9c08-bb19-4cfd-f090-8d01c9ea3b15"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbqlC75hCUgJ"
      },
      "source": [
        "## Make predictions and log metrics to MLFoundry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqjwDFx48QrL",
        "outputId": "489eb2dc-a137-4f9f-e45b-e20a8484e776"
      },
      "outputs": [],
      "source": [
        "y_pred_train = clf.predict(X_train)\n",
        "y_pred_test = clf.predict(X_test)\n",
        "\n",
        "metrics = {\n",
        "    'train/accuracy_score': accuracy_score(y_train, y_pred_train),\n",
        "    'train/f1_weighted': f1_score(y_train, y_pred_train, average='weighted'),\n",
        "    'train/f1_mirco': f1_score(y_train, y_pred_train, average='micro'),\n",
        "    'train/f1_macro': f1_score(y_train, y_pred_train, average='macro'),\n",
        "    'test/accuracy_score': accuracy_score(y_test, y_pred_test),\n",
        "    'test/f1_weighted': f1_score(y_test, y_pred_test, average='weighted'),\n",
        "    'test/f1_mirco': f1_score(y_test, y_pred_test, average='micro'),\n",
        "    'test/f1_macro': f1_score(y_test, y_pred_test, average='macro'),\n",
        "}\n",
        "\n",
        "run.log_metrics(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWzmRW6RCitF"
      },
      "source": [
        "## Log model and end run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqvyHKohOReG",
        "outputId": "116e89aa-6e80-4455-b5c1-f6894e3d0476"
      },
      "outputs": [],
      "source": [
        "run.log_model(clf, framework=mlf.ModelFramework.SKLEARN)\n",
        "run.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXVLO1J0DMjS",
        "outputId": "d497dd30-6c67-4e24-deed-1a16a08e2fe9"
      },
      "outputs": [],
      "source": [
        "print(run.run_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi8QuidwClq6"
      },
      "source": [
        "## Login to servicefoundry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQiqX5_o8TFq",
        "outputId": "797aaf34-db90-49a2-e2e1-f789c60bb49d"
      },
      "outputs": [],
      "source": [
        "import servicefoundry.core as sfy\n",
        "sfy.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gscf6-qBmPNZ"
      },
      "source": [
        "### Create a Servicefoundry workspace\n",
        "\n",
        "A Servicefoundry workspace is a collection of related services that share the same set of permissions.\n",
        "\n",
        "To create a workspace:\n",
        "\n",
        "1. Go to <a href=\"https://app.truefoundry.com/workspace\">ServiceFoundry dashboard</a>\n",
        "\n",
        "2. Click on `Create Workspace` to create a new workspace. **Choose the largest tier available since we will be deploying two services.**\n",
        "\n",
        "3. Once the workspace is created, copy the FQN of the workspace. We shall use this to deploy our webapp and service to the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8ncbYnGd3qd",
        "outputId": "93408650-5e29-495b-fd86-f6e79aab9fc0"
      },
      "outputs": [],
      "source": [
        "WORKSPACE_FQN = input(\"Input workspace FQN copied from the dashboard \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mY6hSo6GjYy"
      },
      "source": [
        "## Create a Python file to deploy as an API Service\n",
        "\n",
        "In this Python file, we write a function that will return the species of iris flower using the model we just trained.\n",
        "\n",
        "ServiceFoundry will automatically create an endpoint out of this function.\n",
        "\n",
        "Notice that we load the model using `run.get_model()` and we used the run id we printed above after training to load the model without having to write any `sklearn` code.\n",
        "\n",
        "\n",
        "### **IMPORTANT**: While running the notebook, replace the `RUN_ID` with your API key and current run id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EBi9-sX9d88",
        "outputId": "81067789-a21c-4c62-a616-3fb11341041c"
      },
      "outputs": [],
      "source": [
        "%%writefile predict.py\n",
        "import os\n",
        "import pandas as pd\n",
        "import mlfoundry as mlf\n",
        "import json\n",
        "\n",
        "RUN_ID = 'e619e9a3243e426aa3aa263c00dc13a4'\n",
        "\n",
        "client = mlf.get_client(api_key=os.environ.get('TFY_API_KEY'))\n",
        "run = client.get_run(RUN_ID)\n",
        "model = run.get_model()\n",
        "\n",
        "def species(features):\n",
        "  features = json.loads(features)\n",
        "  df = pd.DataFrame.from_dict([features])\n",
        "  prediction = model.predict(df)[0]\n",
        "  return ['setosa', 'versicolor', 'virginica'][prediction]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fbK_KzcQDkY",
        "outputId": "22b478d8-9fe8-4cae-a8d4-422b211419d3"
      },
      "outputs": [],
      "source": [
        "requirements = sfy.gather_requirements(\"predict.py\")\n",
        "service = sfy.Service(\"predict.py\", requirements, sfy.Parameters(\n",
        "    name=\"iris-service\",\n",
        "    workspace=WORKSPACE_FQN,\n",
        "    cpu=sfy.CPU(required=1),\n",
        "    memory=sfy.Memory(required=1024 * 1000 * 1000)\n",
        "))\n",
        "\n",
        "service.deploy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JnPHK5FHTfN"
      },
      "source": [
        "## Deploying a Gradio webapp\n",
        "\n",
        "To deploy a Gradio app for the model, we simply assing the Gradio Interface object to a variable called `app`.\n",
        "\n",
        "Once again, we are using the run_id we printed above (while training) to load the model from MLFoundry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9j-0yq52x6O"
      },
      "outputs": [],
      "source": [
        "# install gradio\n",
        "!pip install gradio==3.0.17"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukpjO9xgMhZ8"
      },
      "source": [
        "\n",
        "### **IMPORTANT**: While running the notebook, replace the `RUN_ID` with your API key and current run id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D181soaXpJRC",
        "outputId": "ca848a29-8b0c-4137-f02f-206ba4f1d13a"
      },
      "outputs": [],
      "source": [
        "%%writefile webapp.py\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import mlfoundry as mlf\n",
        "import json\n",
        "\n",
        "RUN_ID = 'e619e9a3243e426aa3aa263c00dc13a4'\n",
        "\n",
        "client = mlf.get_client(api_key=os.environ.get('TFY_API_KEY'))\n",
        "run = client.get_run(RUN_ID)\n",
        "model = run.get_model()\n",
        "\n",
        "def predict_species(f1, f2, f3, f4):\n",
        "    df = pd.DataFrame.from_dict([[f1, f2, f3, f4]])\n",
        "    prediction = model.predict(df)[0]\n",
        "    return ['setosa', 'versicolor', 'virginica'][prediction]\n",
        "\n",
        "examples = [[5.1, 3.5,\t1.4,\t0.2]]\n",
        "app = gr.Interface(fn=predict_species, title=\"Iris Classification\", inputs=[gr.Number(label=\"sepal length (cm)\"), gr.Number(label=\"sepal width (cm)\"), gr.Number(label=\"petal length (cm)\"), gr.Number(label=\"petal width (cm)\")], outputs=[gr.Textbox(label=\"Answer\")], examples=examples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21jPySh7sAnX"
      },
      "outputs": [],
      "source": [
        "requirements = sfy.gather_requirements(\"webapp.py\")\n",
        "webapp = sfy.Gradio(\"webapp.py\", requirements, sfy.Parameters(\n",
        "    name=\"gradio-app\",\n",
        "    workspace=WORKSPACE_FQN,\n",
        "    cpu=sfy.CPU(required=1),\n",
        "    memory=sfy.Memory(required=1024 * 1000 * 1000)\n",
        "))\n",
        "\n",
        "webapp.deploy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vR9n-JH3thn"
      },
      "source": [
        "## Logging Grid Search Results\n",
        "\n",
        "GridSearch can be used to identify the optimal hyper-parameters for your model. Using MLFoundry, we create a project to track various hyper-parameters and the corresponding model performace.\n",
        "\n",
        "**Each run in this project corresponds to a unique set of hyper-parameters.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS0swWKAwrJL",
        "outputId": "41b1f6d1-66d2-4dfe-94d9-7cd188b3aee2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 5, 10]}\n",
        "clf = GridSearchCV(SVC(), parameters)\n",
        "clf.fit(iris.data, iris.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9lTXs2-xv4s"
      },
      "outputs": [],
      "source": [
        "def log_grid_search_results(ml_repo, classifier):\n",
        "  results = classifier.cv_results_\n",
        "  count = len(results['mean_test_score'])\n",
        "  runs = [client.create_run(ml_repo, f'parameter-set-{i+1}') for i in range(0, count)]\n",
        "  [each.set_tags({'run_type': 'grid_search'}) for each in runs]\n",
        "  for i in range(0, count):\n",
        "    runs[i].log_params(results['params'][i])\n",
        "\n",
        "    runs[i].log_metrics({\n",
        "        'rank': results['rank_test_score'][i],\n",
        "        'mean_test_score': results['mean_test_score'][i],\n",
        "        'mean_fit_time': results['mean_fit_time'][i],\n",
        "        'std_score_time': results['std_score_time'][i]\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySAQdaE3xwVg",
        "outputId": "e0b9f5df-cb41-4434-a8e9-6a62e8758edf"
      },
      "outputs": [],
      "source": [
        "log_grid_search_results(ML_REPO, clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRjCrWfQ31D3"
      },
      "source": [
        "## Logging K-fold cross validation scores\n",
        "\n",
        "We use an MLFoundry project to log metrics during k-fold cross validation. \n",
        "\n",
        "**Each run corresponds to a single fold and logs the fold dataset and metrics.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXMO7NdC4RIn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "iris_df = datasets.load_iris(as_frame=True)\n",
        "\n",
        "features = iris_df.data\n",
        "actuals = iris_df.target.apply(lambda class_index: iris_df.target_names[class_index])\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "for i, (train_index, test_index) in enumerate(kf.split(features, y=actuals)):\n",
        "    # create a run named fold-n for the nth-fold\n",
        "    run = client.create_run(ML_REPO, f'fold-{i+1}')\n",
        "    run.set_tags({'run_type': 'cross_validation'})\n",
        "\n",
        "    X_train, X_test = (\n",
        "          features.iloc[train_index],\n",
        "          features.iloc[test_index],\n",
        "      )\n",
        "    y_train, y_test = (\n",
        "          actuals.iloc[train_index],\n",
        "          actuals.iloc[test_index],\n",
        "      )\n",
        "\n",
        "    # log train dataset\n",
        "    run.log_dataset(\n",
        "      features=X_train,\n",
        "      actuals=y_train,\n",
        "      dataset_name=f\"fold-{i + 1}-train\",\n",
        "      only_stats=True,\n",
        "    )\n",
        "\n",
        "    # log test dataset\n",
        "    run.log_dataset(\n",
        "        features=X_test,\n",
        "        actuals=y_test,\n",
        "        dataset_name=f\"fold-{i + 1}-test\",\n",
        "        only_stats=True,\n",
        "    )\n",
        "\n",
        "    # model training\n",
        "    clf = SVC(gamma='scale', kernel='rbf', probability=True, C=1.2)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "\n",
        "    metrics = {\n",
        "        'train/accuracy_score': accuracy_score(y_train, y_pred_train),\n",
        "        'train/f1_weighted': f1_score(y_train, y_pred_train, average='weighted'),\n",
        "        'train/f1_mirco': f1_score(y_train, y_pred_train, average='micro'),\n",
        "        'train/f1_macro': f1_score(y_train, y_pred_train, average='macro'),\n",
        "        'test/accuracy_score': accuracy_score(y_test, y_pred_test),\n",
        "        'test/f1_weighted': f1_score(y_test, y_pred_test, average='weighted'),\n",
        "        'test/f1_mirco': f1_score(y_test, y_pred_test, average='micro'),\n",
        "        'test/f1_macro': f1_score(y_test, y_pred_test, average='macro'),\n",
        "    }\n",
        "\n",
        "    run.log_metrics(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je6IJF_A0iM6"
      },
      "source": [
        "## Logging K-fold cross validation in a single run\n",
        "\n",
        "In the above cell, we logged cross validation metrics for each fold in a separate run. We can also log it in a single run by specifying the `step` argument with `log_metrics`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTiLTNcl0tqj"
      },
      "outputs": [],
      "source": [
        "run = client.create_run(ML_REPO, 'cross-validation-run')\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(kf.split(features, y=actuals)):\n",
        "    X_train, X_test = (\n",
        "          features.iloc[train_index],\n",
        "          features.iloc[test_index],\n",
        "      )\n",
        "    y_train, y_test = (\n",
        "          actuals.iloc[train_index],\n",
        "          actuals.iloc[test_index],\n",
        "      )\n",
        "\n",
        "    # model training\n",
        "    clf = SVC(gamma='scale', kernel='rbf', probability=True, C=1.2)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "\n",
        "    metrics = {\n",
        "        'train/accuracy_score': accuracy_score(y_train, y_pred_train),\n",
        "        'train/f1_weighted': f1_score(y_train, y_pred_train, average='weighted'),\n",
        "        'train/f1_mirco': f1_score(y_train, y_pred_train, average='micro'),\n",
        "        'train/f1_macro': f1_score(y_train, y_pred_train, average='macro'),\n",
        "        'test/accuracy_score': accuracy_score(y_test, y_pred_test),\n",
        "        'test/f1_weighted': f1_score(y_test, y_pred_test, average='weighted'),\n",
        "        'test/f1_mirco': f1_score(y_test, y_pred_test, average='micro'),\n",
        "        'test/f1_macro': f1_score(y_test, y_pred_test, average='macro'),\n",
        "    }\n",
        "\n",
        "    run.log_metrics(metrics, i) # pass i as second argument `step`\n",
        "\n",
        "run.end()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7vR9n-JH3thn",
        "DRjCrWfQ31D3",
        "Je6IJF_A0iM6"
      ],
      "name": "iris-truefoundry-demo",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
